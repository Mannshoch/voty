#!/usr/bin/env python3
# load latest dump from ftp

from datetime import datetime
from ftplib import FTP_TLS
import os
import sys
import time
import re

CONTAINER = "postgres"
FILE = "voty-prod-latest-data.dump"
DESTINATION = "/tmp/"+FILE
ENV_FILE = "/voty/.env"
HOSTNAME = os.uname()[1]


def readEnv(envFile):
    lines = open(envFile).readlines()
    # remove newlines
    stripped = map(lambda s: s.replace("\n", ""), lines)
    # filter empty lines and comments
    filtered = filter(lambda s: len(s) > 0 and not s.startswith("#"), stripped)
    pairs = map(lambda s: re.split(r'\s*=\s*', s), filtered)
    return dict(pairs)


env = readEnv(ENV_FILE)
if (not 'RESTORE_SERVER' in env):
    print("Skipping backup, as RESTORE_SERVER is not defined")
    sys.exit(0)

RESTORE_SERVER = env['RESTORE_SERVER']
RESTORE_USER = env['RESTORE_USER']
RESTORE_PASS = env['RESTORE_PASS']


# Download latest backup via FTPES
handle = open(DESTINATION, 'wb')
ftps = FTP_TLS(RESTORE_SERVER)
ftps.login(RESTORE_USER, RESTORE_PASS)

ftps.prot_p()
ftps.retrbinary('RETR %s' % FILE, handle.write)

DATABASE_URL = env['DATABASE_URL']

match = re.match(r'.*//(.*?)\:(.*?)@(.*?)\/(.*)', DATABASE_URL)
if (not match):
    print("Error: could not parse DATABASE_URL: %s" % DATABASE_URL)
    exit(1)

db = match.groups()
DB_USER = db[0]
DB_PASS = db[1]
DB_HOST = db[2]
DB_NAME = db[3]


# Run db dump over docker
""" dbCmd = "pg_dump -U %s %s -Fc" % (DB_USER, DB_NAME)
cmd = "docker exec %s %s > %s/%s" % (
    CONTAINER, dbCmd, BACKUPROOT, fileName)

code = os.system(cmd)
if (code != 0):
    print("Error: could not run pg_dump via docker")
    sys.exit(code)
 """
